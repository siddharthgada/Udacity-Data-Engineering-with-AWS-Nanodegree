# ğŸ¶ Data Pipelines with Apache Airflow â€“ Sparkify ETL Automation
This project automates the data pipeline for Sparkify, a fictional music streaming company, using Apache Airflow to orchestrate and monitor the ETL process. The pipeline extracts user activity and song data from AWS S3, stages it in Amazon Redshift, transforms it into a star-schema, and performs data quality checks for analytical reporting.

## ğŸš€ Project Goals:<br>
- Design and implement dynamic, reusable, and monitorable ETL pipelines using Airflow
- Load log and song data from S3 into Redshift staging tables
- Transform staged data into star-schema fact and dimension tables
- Implement custom Airflow operators for staging, loading and data quality validation
- Enable backfilling, incremental loads and data quality monitoring

## ğŸ› ï¸ Tools & Technologies: <br>
- Apache Airflow â€“ For orchestration and scheduling of ETL pipelines
- Amazon Redshift Serverless â€“ As the data warehouse for storing staging, fact and dimension tables
- Amazon S3 â€“ For storing raw JSON log and song data
- AWS IAM â€“ For managing access and credentials
- Python â€“ For writing custom Airflow operators and scripts
- PostgresHook (Airflow) â€“ To interact with Redshift using SQL
- VS Code â€“ Primary IDE for developing DAGs, custom operators and project configuration
- Airflow UI â€“ For monitoring DAG runs, viewing logs and triggering tasks manually

## ğŸ“ Project Structure:<br>
. <br>
â”œâ”€â”€ dags/<br>
â”‚   â””â”€â”€ sparkify_dag.py  <br>            # Main Airflow DAG
â”œâ”€â”€ plugins/<br>
â”‚   â””â”€â”€ helpers/<br>
â”‚       â””â”€â”€ sql_queries.py <br>         # SQL transformations
â”‚   â””â”€â”€ operators/<br>
â”‚       â”œâ”€â”€ stage_redshift.py <br>      # StageToRedshiftOperator
â”‚       â”œâ”€â”€ load_fact.py      <br>      # LoadFactOperator
â”‚       â”œâ”€â”€ load_dimension.py  <br>     # LoadDimensionOperator
â”‚       â””â”€â”€ data_quality.py    <br>     # DataQualityOperator
â”œâ”€â”€ final_project_dag_graph.jpg <br>    # DAG visualization
â””â”€â”€ Readme.md<br>

## âœ… Custom Airflow Operators: <br>
|Operator	| Purpose|
|:---:|:---:|
| StageToRedshiftOperator	| Loads JSON data from S3 into Redshift staging tables using COPY command. |
| LoadFactOperator	| Loads data into songplays fact table using append-only strategy. |
| LoadDimensionOperator |	Loads data into dimension tables (users, songs, artists, time) with truncate or append option. |
| DataQualityOperator	| Runs checks to validate table row counts and NULL constraints. |

## ğŸ” Data Quality Checks: <br>
Each dimension table is validated for:
- No NULLs in any column of any dimension table

## ğŸ”„ Integration with AWS Services: <br>
- Data is copied from Udacityâ€™s S3 bucket to a user-owned S3 bucket
- Amazon Redshift Serverless is configured to ingest data using the COPY command from the S3 bucket
- Airflow connects to:
  - Redshift Serverless using the Postgres hook with a connection ID (e.g., redshift)
  - AWS using AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY through Airflowâ€™s connections UI or environment variables

## ğŸ—ƒï¸ Datasets: <br>
- Log Data: s3://udacity-dend/log_data
- Song Data: s3://udacity-dend/song-data
- JSON Path: log_json_path.json

NOTE: These datasets can be copied onto your own S3 bucket (in the same AWS region as Redshift Serverless) for optimal performance. - I did this.

## âš™ï¸ DAG Configuration: <br>
- DAG default arguments:
  - 'start_date': pendulum.now()
  - 'retries': 3
  - 'retry_delay': timedelta(minutes=5)
  - 'depends_on_past': False
  - 'catchup': False
  - 'email_on_retry': False
  - 'email_on_failure': False

## ğŸ§‘â€ğŸ’» How to Run:
- Create IAM User (to give access to Airflow UI)
- IAM User needs to have the following existing policies:
  - AdministratorAccess
  - AmazonRedshiftFullAccess
  - AmazonS3FullAccess
- Create an Access Key so that the credentials can be used to configure the AWS connection in Airflow
- Create IAM Role (to give full access to S3)
- IAM Role needs to have the following existing policies:
  - AmazonS3FullAccess
- Create a Redshift Serverless workspace and namespace
  - Associate IAM role created while configuring the workspace and namespace
  - Turn on enhanced VPC routing
  - Edit the workspace to be publicly accessible
  - Add an inbound rule to the VPC Security Group in EC2
    - Type = Custom TCP
    - Port range = 0 - 5500
    - Source = Anywhere-iPv4
- Configure S3 and copy source data to your own bucket
- Set up Airflow connections:
  - AWS credentials (Airflow Connections to AWS Account using the IAM User Access Key)
  - Redshift connection (Airflow Connections to AWS Redshift using workspace endpoint)
- Edit the S3_bucket and S3_key in the DAG to the bucket and key you created and have loaded the files in
- Run the DAG from Airflow UI
- Monitor task execution and logs

## ğŸ’¡ Key Learnings: <br>
1. Dynamic DAG Design
Learned how to create dynamic and modular DAGs in Apache Airflow using reusable custom operators, which improved maintainability and scalability.
2. Custom Airflow Operators
Built custom Airflow operators to encapsulate business logic for staging, loading fact and dimension tables and performing data quality checks.
3. Amazon Redshift Serverless Integration
Gained experience configuring and using Redshift Serverless for loading and querying large datasets, and connecting Redshift to Airflow using Postgres hooks.
4. Incremental Data Loads
Implemented logic for append vs truncate loading strategies, with parameterized controls in dimension load operators to support historical and fresh data loads.
5. Data Quality Validation
Developed reusable data quality checks to ensure reliability of data pipelines, including null checks and row count assertions.
6. End-to-End AWS Workflow
Understood how various AWS services (S3, IAM and Redshift Serverless) work together in a data pipeline, including security and access configurations.

## ğŸ“ˆ Outcome: <br>
By implementing this project, I developed and deployed a fully-automated and monitored ETL pipeline using Airflow, with dynamic tasks, reusable operators and strong data validation to ensure reliable reporting.

## ğŸ‘¤ Author

**Siddharth Gada**  
ğŸ“§ Email: gadasiddharth@gmail.com <br>
ğŸ”— LinkedIn: https://www.linkedin.com/in/siddharthgada/
